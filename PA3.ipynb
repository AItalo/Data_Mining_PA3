{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this assignment, our data comes from about 4.5 thousand inpatient rehabilitation patients. The attributes for each instance are as follows:\n",
    "\n",
    "* ID *(integer)*: Index of the dataset. Counting numbers starting at 0.\n",
    "* Gender *(string)*: Gender of the patient, \"M\" for male and \"F\" for female.\n",
    "* Age *(integer)*: Age of the patient in years\n",
    "* Marital Status *(string)*: Description of the patient's marital status.\n",
    "* RIC *(integer)*: RIC of the patient assigned according to [Appendix B in the Centers for Medicaid and Medicare Services IRF-PAI training manual.](https://www.cms.gov/medicare/medicare-fee-for-service-payment/inpatientrehabfacpps/downloads/irfpai-manual-2012.pd)\n",
    "* Admission Total FIM Score *(integer)*: The admission total Functional Independence Measure (FIM) score of the patient.\n",
    "    * The FIM is a clinical assessment used to measure patient functioning at inpatient rehabilitation hospitals. The FIM is measured at two distinct points in time: admission and discharge.\n",
    "    * The FIM measures the level of assistance required to perform 18 ADL tasks.\n",
    "    * The tasks are categorized as either motor (13 tasks) or cognitive (5 tasks). Each task is scored on a 7-point ordinal scale to measure independence as determined by the amount of assistance required to perform each ADL task.\n",
    "    * For more information about the FIM, see Section III in the [Centers for Medicaid and Medicare Services IRF-PAI training manual.](https://www.cms.gov/medicare/medicare-fee-for-service-payment/inpatientrehabfacpps/downloads/irfpai-manual-2012.pdf)\n",
    "* Discharge Total FIM Score *(integer)*: The discharge total FIM score of the patient.\n",
    "\n",
    "\n",
    "*(Referenced from [PA3](https://github.com/GonzagaCPSC310/PAs/blob/master/PA3.ipynb))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to pull the data from the Github repository it's hosted in. We do this with the following `get_text()` function, which will make a get request from the raw data URL and return the text.\n",
    "\n",
    "Next, we need to put the data into a 2D list for easy manipulation. After pulling the raw text using `get_text()`, we will pass the result into the `create_dataset()` function which will:\n",
    "1. Split the data by each instance based on new lines\n",
    "2. Pop and store the first line of headers\n",
    "3. Split each instance by its elements based on commas\n",
    "4. Replace all numeric attributes with integers\n",
    "5. Return the headers and the 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_text(url):\n",
    "    # code referenced from https://stackoverflow.com/questions/14120502/how-to-download-and-write-a-file-from-github-using-requests\n",
    "    r = requests.get(url)\n",
    "    text = r.text\n",
    "    return text\n",
    "\n",
    "def create_dataset(text):\n",
    "    data_raw = text.split(\"\\r\\n\")\n",
    "    data_raw.pop()\n",
    "    headers = data_raw.pop(0)\n",
    "    for i in range(len(data_raw)):\n",
    "        data_raw[i] = data_raw[i].split(\",\")\n",
    "        if len(data_raw[i]) != 7:\n",
    "            ms = data_raw[i][3] + data_raw[i][4]\n",
    "            data_raw[i][3] = ms\n",
    "            data_raw[i].pop(4)\n",
    "    data = []\n",
    "    for line in data_raw:\n",
    "        instance = []\n",
    "        for elem in line:\n",
    "            try:\n",
    "                instance.append(int(elem))\n",
    "            except:\n",
    "                instance.append(elem)\n",
    "                \n",
    "        data.append(instance)\n",
    "    \n",
    "    return headers, data\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GonzagaCPSC310/PAs/master/files/patient_data_to_clean.csv\"\n",
    "headers, data = create_dataset(get_text(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Cleaning\n",
    "\n",
    "After creating the dataset, we need to clean certain attributes. For this dataset, we will be cleaining up Marital Status and RIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marital Status\n",
    "\n",
    "The marital status attribute of the dataset does not enforce a strict coding system, and so while the majority of the data is well formed, there are many odd entries that don't make for clean classification. Since the physician entering the data could put anything for this field, there is no standard format, meaning that there are many spelling mistakes, capitalization or punctuation differences, and unique or odd entries. \n",
    "\n",
    "For our purposes, we would prefer this data to adhere to a strict coding system, so we will attempt to map each entry to one of 5 different marital statuses. These classifications are:\n",
    "* Never Married\n",
    "* Divorced\n",
    "* Married\n",
    "* Widowed\n",
    "* Separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to clean the data, we must first see what data is initially available. To do this, I first iterated through the dataset and put the marital status of each entry into a set. This ensures uniqueness, and allows me to see a shortened list of each different possible entry in the dataset without having to see all 4.5 thousand entries. I then restrict each entry to only ascii letters and cast each string into its lowercase form, which eliminates errors such as a trailing period or the difference between \"married\", \"Married\", and \"mARRIED\", all of which show up in the raw data. From here, entries are generally one of two categories: Typos, as in \"marrired\" instead of \"married\", or variations on a classification, such as \"unmarried\" or \"single\" for \"never married.\"\n",
    "\n",
    "To deal with typos, I've implemented several rules that cover each typo case in the generated set. These are rules such as strings containing \"wid\" are cast to \"widowed\", strings containing \"marr\" or \"ried\" but **not** containing \"never\" being cast to \"married\", etc.\n",
    "\n",
    "Dealing with variations and unique entries was more difficult, and required rules that were more specific to the dataset. For example, I saw that the set contained \"wife\" and \"so\", meaning significant other. So, I cast these to \"married\". Or, in one case, an entry was of the form \"d x 5 years\", denoting that the patient was divorced for 5 years. In this case, I implemented the rule that if the first letter of the status is \"d\", it will be cast to \"divorced\". The trickiest of these was \"never married\", as there were several different ways physicians indicated this, including \"single\", \"girlfriend\", \"engaged\", \"no\", etc. Because of this, I implemented a catch-all rule as a base case, where if the given status is not cast based on one of the earlier rules, it is automatically determined to be \"never married.\" While this could cause problems if the dataset is expanded, I tested the generated set with all the previous rules implemented and all of the remaining variances were different ways of indicating \"never married\", so in this case the explicit rules are enough that this base case does do what we want it to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the set again and seeing that all marital statuses were successfully cast to one of the 5 desired classifications, I eliminated the set creation and instead changed the attribute in each instance of the database to the correct classification based on the created rules set. This is all handled via the `clean_marital_status()` function, which will return the dataset after marital status has been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "\n",
    "def clean_marital_status(data):\n",
    "    terms = {\"married\", \"divorced\", \"widowed\", \"never married\", \"separated\"}\n",
    "    for instance in data:\n",
    "        status = instance[3]\n",
    "        # line referenced from https://stackoverflow.com/questions/1276764/stripping-everything-but-alphanumeric-chars-from-a-string-in-python\n",
    "        status = \"\".join([ch for ch in status if ch in (ascii_letters + \" \")])\n",
    "        status = status.lower()\n",
    "        \n",
    "        # rules for cleaning marital status strings\n",
    "        if status in terms:\n",
    "            status = status\n",
    "        elif \"wid\" in status:\n",
    "            status = \"widowed\"\n",
    "        elif \"div\" in status:\n",
    "            status = \"divorced\"\n",
    "        elif \"sing\" in status or \"un\" in status or \"not\" in status or \"never\" in status or \"no\" in status:\n",
    "            status = \"never married\"\n",
    "        elif \"wife\" in status or \"husband\" in status or \"sig\" in status or \"so\" in status:\n",
    "            status = \"married\"\n",
    "        elif (\"marr\" in status or \"ried\" in status) and (\"never\" not in status):\n",
    "            status = \"married\"\n",
    "        elif status[0] == \"d\":\n",
    "            status = \"divorced\"\n",
    "\n",
    "        else:\n",
    "            status = \"never married\"\n",
    "        \n",
    "        instance[3] = status\n",
    "    \n",
    "    return data\n",
    "        \n",
    "\n",
    "    \n",
    "data = clean_marital_status(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIC\n",
    "\n",
    "Cleaning the RIC data is much easier than cleaning the marital status entries, since the raw data does adhere to a standard. For this attribute, we simply create a decoder dictionary *(Provided in [PA3](https://github.com/GonzagaCPSC310/PAs/blob/master/PA3.ipynb))* and use it to cast the integer values of the RIC attribute to a string representation. This is handled in `clean_RIC()`, which again returns the dataset after converting this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that I originally decided to test this cleaning process with an `If` statement and printing out \"Error\" if it couldn't translate the RIC score to the decoded value. I expected to never get an \"Error\" printout, but to my surprise there were 5. Exploring further, I printed out the ID of each of these instances, as well as what their RIC was, and was very confused when each of the errored values was a string. Going back, I realized that when I split each instance into it's elements in `create_dataset()`, it was also splitting cases in which the Marital Status contained a comma, and so the problem wasn't that the RIC was entered improperly, but that the element at index 4 of the instance was the second part of the marital status attribute.\n",
    "\n",
    "After going back and handling this case in the `create_dataset()` function, I reran the test and received no \"Error\" warnings, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_RIC(data):\n",
    "    # decoder referenced from PA3 Instructions\n",
    "    ric_decoder = {1: \"Stroke\", \n",
    "                   2: \"TBI\", \n",
    "                   3: \"NTBI\", \n",
    "                   4: \"TSCI\", \n",
    "                   5: \"NTSCI\", \n",
    "                   6: \"Neuro\", \n",
    "                   7: \"FracLE\", \n",
    "                   8: \"ReplLE\", \n",
    "                   9: \"Ortho\", \n",
    "                   10: \"AMPLE\", \n",
    "                   11: \"AMP-NLE\", \n",
    "                   12: \"OsteoA\", \n",
    "                   13: \"RheumA\", \n",
    "                   14: \"Cardiac\", \n",
    "                   15: \"Pulmonary\", \n",
    "                   16: \"Pain\", \n",
    "                   17: \"MMT-NBSCI\", \n",
    "                   18: \"MMT-BSCI\", \n",
    "                   19: \"GB\", \n",
    "                   20: \"Misc\", \n",
    "                   21: \"Burns\"}\n",
    "    for instance in data:\n",
    "        ric = instance[4]\n",
    "        ric_d = ric_decoder[ric]\n",
    "        instance[4] = ric_d\n",
    "    return data\n",
    "            \n",
    "data = clean_RIC(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data, we will write the dataset out to a file, \"patient_data_cleaned.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_csv(data, filename, headers=\"\"):\n",
    "    f = open(filename, \"w\")\n",
    "    if headers != \"\":\n",
    "        f.write(headers + \"\\n\")\n",
    "    for instance in data:\n",
    "        instance_str = []\n",
    "        for elem in instance:\n",
    "            instance_str.append(str(elem))\n",
    "        write_str = \",\".join(instance_str)\n",
    "        write_str += \"\\n\"\n",
    "        f.write(write_str)\n",
    "    f.close()\n",
    "    \n",
    "write_data_to_csv(data, \"patient_data_cleaned.csv\", headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Statistics\n",
    "\n",
    "For this step, we will calculate several statistics of the dataset generated by Step 1. The statistics we will compute are as follows:\n",
    "\n",
    "1. Total number of patients\n",
    "2. Total number of males\n",
    "3. Total number of females\n",
    "4. Total number of married patients\n",
    "5. RIC label for the most commonly occurring RIC\n",
    "6. Total number of patients with the most commonly occurring RIC\n",
    "7. Average age for stroke patients\n",
    "8. Standard deviation of age for stroke patients\n",
    "9. Average age for male stroke patients\n",
    "10. Standard deviation of age for male stroke patients\n",
    "11. Average age for female stroke patients\n",
    "12. Standard deviation of age for female stroke patients\n",
    "\n",
    "To do this, we will first construct several generalized helper functions. These functions are:\n",
    "\n",
    "* `compute_total_instances()`  \n",
    "    * **Params**: \n",
    "        * `data` = the dataset to query\n",
    "    * **Returns**: The number of instances in that dataset\n",
    "* `compute_attribute_frequency()`\n",
    "    * **Params**: \n",
    "        * `data` = the dataset to query\n",
    "        * `index` = the index of the attribute to query\n",
    "        * `value` = the value to calculate the frequency for\n",
    "    * **Returns**: Frequency of the given value for the given attribute in the dataset\n",
    "* `compute_most_common_element()`\n",
    "    * **Params**: \n",
    "        * `data` = the dataset to query\n",
    "        * `index` = the index of the attribute to query\n",
    "    * **Returns**: The most commonly occurring element of that attribute in the dataset\n",
    "* `compute_dependent_statistics()`\n",
    "    * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `stat_index` = the index of the attribute to calculate statistics for\n",
    "        * `depend_indices` = list of indices for restricting attributes\n",
    "        * `depend_values` = list of values for restricting attributes to match for\n",
    "    * **Returns**: Average and Standard Deviation of the desired attribute for all instances whose restricting attributes match the given values\n",
    "\n",
    "Additionally, the formulas we will use are as follows:\n",
    "\n",
    "* **Average**: $\\frac{\\sum_{i=1}^{n} x}{n}$\n",
    "* **Standard Deviation**: $\\sqrt{\\frac{\\sum_{i=1}^{n} (x_{i} - \\overline{x})^{2}}{n}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "def compute_total_instances(data):\n",
    "    return len(data)\n",
    "\n",
    "def compute_attribute_frequency(data, index, value):\n",
    "    freq = 0\n",
    "    for instance in data:\n",
    "        if instance[index] == value:\n",
    "            freq += 1\n",
    "    return freq\n",
    "\n",
    "def compute_most_common_element(data, index):\n",
    "    freqs = {}\n",
    "    for instance in data:\n",
    "        if instance[index] in freqs:\n",
    "            freqs[instance[index]] += 1\n",
    "        else:\n",
    "            freqs[instance[index]] = 1            \n",
    "    # line referenced from https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "    mce = max(freqs.items(), key=operator.itemgetter(1))[0]\n",
    "    return mce\n",
    "\n",
    "def compute_dependent_statistics(data, stat_index, depend_indices, depend_values):\n",
    "    values = []\n",
    "    for instance in data:\n",
    "        match_dependencies = True\n",
    "        for i in range(len(depend_indices)):\n",
    "            if instance[depend_indices[i]] != depend_values[i]:\n",
    "                match_dependencies = False\n",
    "        if match_dependencies == True:\n",
    "            values.append(instance[stat_index])\n",
    "    n = len(values)\n",
    "    avg = sum(values) / n\n",
    "    variances = []\n",
    "    for x in values:\n",
    "        variances.append((x - avg) ** 2)\n",
    "    variance = sum(variances) / n\n",
    "    std = np.sqrt(variance)\n",
    "    return avg, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these helper functions defined, we can determine which function will be used for each desired statistic.\n",
    "\n",
    "1. Total number of patients\n",
    "    * `compute_total_instances()`\n",
    "2. Total number of males\n",
    "    * `compute_attribute_frequency()`\n",
    "3. Total number of females\n",
    "    * `compute_attribute_frequency()`\n",
    "4. Total number of married patients\n",
    "    * `compute_attribute_frequency()`\n",
    "5. RIC label for the most commonly occurring RIC\n",
    "    * `compute_most_common_element()`\n",
    "6. Total number of patients with the most commonly occurring RIC\n",
    "    * `compute_attribute_frequency()` *and* `compute_most_common_element()`\n",
    "7. Average age for stroke patients\n",
    "    * `compute_dependent_statistics()`\n",
    "8. Standard deviation of age for stroke patients\n",
    "    * `compute_dependent_statistics()`\n",
    "9. Average age for male stroke patients\n",
    "    * `compute_dependent_statistics()`\n",
    "10. Standard deviation of age for male stroke patients\n",
    "    * `compute_dependent_statistics()`\n",
    "11. Average age for female stroke patients\n",
    "    * `compute_dependent_statistics()`\n",
    "12. Standard deviation of age for female stroke patients\n",
    "    * `compute_dependent_statistics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Total number of patients', 4555],\n",
       " ['Total number of males', 2242],\n",
       " ['Total number of females', 2313],\n",
       " ['Total number of married patients', 2268],\n",
       " ['RIC label for the most commonly occurring RIC', 'Stroke'],\n",
       " ['Total number of patients with the most commonly occurring RIC', 1169],\n",
       " ['Average age for stroke patients', 71.29255774165954],\n",
       " ['Standard deviation of age for stroke patients', 14.334747402979598],\n",
       " ['Average age for male stroke patients', 70.07742998352553],\n",
       " ['Standard deviation of age for male stroke patients', 13.830067809038166],\n",
       " ['Average age for male stroke patients', 72.6049822064057],\n",
       " ['Standard deviation of age for male stroke patients', 14.748563236002816]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = []\n",
    "\n",
    "statistics.append([\"Total number of patients\", compute_total_instances(data)])\n",
    "statistics.append([\"Total number of males\", compute_attribute_frequency(data, 1, \"M\")])\n",
    "statistics.append([\"Total number of females\", compute_attribute_frequency(data, 1, \"F\")])\n",
    "statistics.append([\"Total number of married patients\", compute_attribute_frequency(data, 3, \"married\")])\n",
    "statistics.append([\"RIC label for the most commonly occurring RIC\", compute_most_common_element(data, 4)])\n",
    "statistics.append([\"Total number of patients with the most commonly occurring RIC\", compute_attribute_frequency(data, 4, compute_most_common_element(data, 4))])\n",
    "avg7, std8 = compute_dependent_statistics(data, 2, [4], [\"Stroke\"])\n",
    "statistics.append([\"Average age for stroke patients\", avg7])\n",
    "statistics.append([\"Standard deviation of age for stroke patients\", std8])\n",
    "avg9, std10 = compute_dependent_statistics(data, 2, [1, 4], [\"M\", \"Stroke\"])\n",
    "statistics.append([\"Average age for male stroke patients\", avg9])\n",
    "statistics.append([\"Standard deviation of age for male stroke patients\", std10])\n",
    "avg11, std12 = compute_dependent_statistics(data, 2, [1, 4], [\"F\", \"Stroke\"])\n",
    "statistics.append([\"Average age for male stroke patients\", avg11])\n",
    "statistics.append([\"Standard deviation of age for male stroke patients\", std12])\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
